{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_files = sc.parallelize(glob.glob(\"../../enron1/ham/*.txt\"),10)\n",
    "spam_files = sc.parallelize(glob.glob(\"../../enron1/spam/*.txt\"),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_to_str(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "ham = ham_files.map(lambda f: (path.basename(f), file_to_str(f)))\n",
    "spam = spam_files.map(lambda f: (path.basename(f), file_to_str(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            filename|label|                text|\n",
      "+--------------------+-----+--------------------+\n",
      "|0001.1999-12-10.f...|  0.0|[Subject:, christ...|\n",
      "|0002.1999-12-13.f...|  0.0|[Subject:, vastar...|\n",
      "|0003.1999-12-14.f...|  0.0|[Subject:, calpin...|\n",
      "|0004.1999-12-14.f...|  0.0|[Subject:, re, :,...|\n",
      "|0005.1999-12-14.f...|  0.0|[Subject:, meter,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "df_ham = ham.map(lambda (f,t):Row(label=0.0, filename=f, text=t.split())).toDF().cache()\n",
    "df_spam = spam.map(lambda (f,t):Row(label=1.0, filename=f, text=t.split())).toDF().cache()\n",
    "df_data = df_ham.union(df_spam)\n",
    "df_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|            filename|label|                text|                  tf|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|0001.1999-12-10.f...|  0.0|[Subject:, christ...|(5,[1,2,3,4],[2.0...|\n",
      "|0002.1999-12-13.f...|  0.0|[Subject:, vastar...|(5,[0,1,2,3,4],[1...|\n",
      "|0003.1999-12-14.f...|  0.0|[Subject:, calpin...|(5,[0,1,2,4],[2.0...|\n",
      "|0004.1999-12-14.f...|  0.0|[Subject:, re, :,...|(5,[0,1,2,3,4],[4...|\n",
      "|0005.1999-12-14.f...|  0.0|[Subject:, meter,...|(5,[0,1,2,3,4],[4...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(numFeatures=5, inputCol=\"text\", outputCol=\"tf\")\n",
    "df_data_tf = hashingTF.transform(df_data)\n",
    "df_data_tf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|            filename|label|                text|                  tf|                 idf|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|0001.1999-12-10.f...|  0.0|[Subject:, christ...|(5,[1,2,3,4],[2.0...|(5,[1,2,3,4],[0.0...|\n",
      "|0002.1999-12-13.f...|  0.0|[Subject:, vastar...|(5,[0,1,2,3,4],[1...|(5,[0,1,2,3,4],[1...|\n",
      "|0003.1999-12-14.f...|  0.0|[Subject:, calpin...|(5,[0,1,2,4],[2.0...|(5,[0,1,2,4],[0.0...|\n",
      "|0004.1999-12-14.f...|  0.0|[Subject:, re, :,...|(5,[0,1,2,3,4],[4...|(5,[0,1,2,3,4],[0...|\n",
      "|0005.1999-12-14.f...|  0.0|[Subject:, meter,...|(5,[0,1,2,3,4],[4...|(5,[0,1,2,3,4],[0...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(minDocFreq=3, inputCol=\"tf\", outputCol=\"idf\")\n",
    "model = idf.fit(df_data_tf)\n",
    "df_data_tf_idf = model.transform(df_data_tf)\n",
    "df_data_tf_idf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "(trainData, testData) = df_data_tf_idf.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'idf')\n",
    "model = rf.fit(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|            filename|label|                text|                  tf|                 idf|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|0002.1999-12-13.f...|  0.0|[Subject:, vastar...|(5,[0,1,2,3,4],[1...|(5,[0,1,2,3,4],[1...|[12.5720286448161...|[0.62860143224080...|       0.0|\n",
      "|0003.1999-12-14.f...|  0.0|[Subject:, calpin...|(5,[0,1,2,4],[2.0...|(5,[0,1,2,4],[0.0...|[15.3284586218421...|[0.76642293109210...|       0.0|\n",
      "|0005.1999-12-14.f...|  0.0|[Subject:, meter,...|(5,[0,1,2,3,4],[4...|(5,[0,1,2,3,4],[0...|[15.8818467289343...|[0.79409233644671...|       0.0|\n",
      "|0007.1999-12-14.f...|  0.0|[Subject:, mcmull...|(5,[0,1,2,3,4],[3...|(5,[0,1,2,3,4],[0...|[13.4509241509410...|[0.67254620754705...|       0.0|\n",
      "|0009.1999-12-14.f...|  0.0|[Subject:, meter,...|(5,[0,1,2,3,4],[4...|(5,[0,1,2,3,4],[0...|[13.7424692987704...|[0.68712346493852...|       0.0|\n",
      "|0010.1999-12-14.f...|  0.0|[Subject:, duns, ...|(5,[0,1,2,3,4],[5...|(5,[0,1,2,3,4],[0...|[16.0485584772508...|[0.80242792386254...|       0.0|\n",
      "|0011.1999-12-14.f...|  0.0|[Subject:, king, ...|(5,[0,1,2,3,4],[9...|(5,[0,1,2,3,4],[0...|[16.0617355867966...|[0.80308677933983...|       0.0|\n",
      "|0014.1999-12-15.f...|  0.0|[Subject:, lst, r...|(5,[0,1,2,3,4],[3...|(5,[0,1,2,3,4],[0...|[16.2065220536096...|[0.81032610268048...|       0.0|\n",
      "|0016.1999-12-15.f...|  0.0|[Subject:, unify,...|(5,[0,1,2,3,4],[3...|(5,[0,1,2,3,4],[0...|[13.6717262147308...|[0.68358631073654...|       0.0|\n",
      "|0019.1999-12-15.f...|  0.0|[Subject:, meter,...|(5,[0,1,2,3,4],[6...|(5,[0,1,2,3,4],[0...|[13.7424692987704...|[0.68712346493852...|       0.0|\n",
      "|0020.1999-12-15.f...|  0.0|[Subject:, meter,...|(5,[0,1,2,3,4],[1...|(5,[0,1,2,3,4],[0...|[13.2887646435138...|[0.66443823217569...|       0.0|\n",
      "|0021.1999-12-15.f...|  0.0|[Subject:, y, 2, ...|(5,[0,1,2,3,4],[1...|(5,[0,1,2,3,4],[0...|[13.3388891866514...|[0.66694445933257...|       0.0|\n",
      "|0022.1999-12-16.f...|  0.0|[Subject:, re, :,...|(5,[0,1,2,3,4],[4...|(5,[0,1,2,3,4],[0...|[13.3462016358705...|[0.66731008179353...|       0.0|\n",
      "|0023.1999-12-16.f...|  0.0|[Subject:, hpl, f...|(5,[0,1,2,3,4],[4...|(5,[0,1,2,3,4],[0...|[16.0608909856485...|[0.80304454928242...|       0.0|\n",
      "|0028.1999-12-17.f...|  0.0|[Subject:, pennze...|(5,[0,1,2,3,4],[1...|(5,[0,1,2,3,4],[0...|[13.6965017709176...|[0.68482508854588...|       0.0|\n",
      "|0031.1999-12-20.f...|  0.0|[Subject:, out, o...|(5,[0,1,2,3,4],[2...|(5,[0,1,2,3,4],[0...|[13.5927798577766...|[0.67963899288883...|       0.0|\n",
      "|0034.1999-12-20.f...|  0.0|[Subject:, re, :,...|(5,[0,1,2,3,4],[5...|(5,[0,1,2,3,4],[0...|[16.1967981685939...|[0.80983990842969...|       0.0|\n",
      "|0035.1999-12-20.f...|  0.0|[Subject:, re, :,...|(5,[0,1,2,3,4],[4...|(5,[0,1,2,3,4],[0...|[16.2251616392808...|[0.81125808196404...|       0.0|\n",
      "|0039.1999-12-21.f...|  0.0|[Subject:, valero...|(5,[0,1,2,3,4],[1...|(5,[0,1,2,3,4],[0...|[13.6965017709176...|[0.68482508854588...|       0.0|\n",
      "|0044.1999-12-21.f...|  0.0|[Subject:, re, :,...|(5,[0,1,2,3,4],[7...|(5,[0,1,2,3,4],[0...|[16.2990433044964...|[0.81495216522482...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.transform(testData)\n",
    "test_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7344332232268187"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
